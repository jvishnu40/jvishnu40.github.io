---
title:      Sending AWS RDS log files to S3 bucket
layout:     post
category:   Tutorial
tags: 	    [aws, log rotation]
#feature:   /assets/img/1_tsunami_udp_data_transfer.svg
---
Using AWS RDS has added ease of database infrastructure setup and administration. Still as an Ops engineer you need to manage the database log files and archive them to storage. In this post I will discuss backing up RDS logfile to S3 bucket. This post is written not focusing on tight AWS security principles (such as not using AWS CLI key in EC2 instances) rather focused on general ideas to make the backup process succeed.

<!--more-->

First we need to create an IAM group in AWS with RDS and S3 full access. Follow the screenshot below,

![Create IAM Group](/assets/img/2018-04-22-01.png)*Create new IAM group*

I am using AmazonRDSFullAccess and AmazonS3FullAccess policies. Although its not recommended to use S3 full access policy but I am trying to implement a basic general idea of backup process. Next create IAM user with AWS access key. We will be using this access key in aws CLI client (instead of using access credential in EC2 use IAM role while creating EC2 instance for better security). Next, install aws cli using pip and configure aws cli using `aws configure` command.


```
#!/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/home/ubuntu

INSTANCE='rds-db-identifier'
BUCKET='backup-db-log-bucket'

mkdir -p ${INSTANCE} && cd ${INSTANCE}
for i in `/home/ubuntu/.local/bin/aws rds describe-db-log-files --db-instance-identifier ${INSTANCE} --output text | awk '{print $3}' | sed '$d' | tail -n 10` ; do
	FILE=`basename ${i}`
	ARCHIVE=${FILE}.tar.gz
	if [ ! -e ${ARCHIVE} ]; then
		echo "Downloading ${i} ........."
		`which aws` rds download-db-log-file-portion --db-instance-identifier ${INSTANCE} --log-file-name ${i} --starting-token 0 --output text > ${FILE}
		tar -cvzf ${ARCHIVE} ${FILE}
		echo "Uploading to S3 bucket ........"
		`which aws` s3 mv ${ARCHIVE} s3://${BUCKET}/
		rm ${FILE}
	fi
done

##
#	Additionally you can run a cronjob every 45 minutes to upload db log to S3
##
#	*/45 * * * * /bin/bash /home/ubuntu/.crontasks/script >/dev/null 2>&1
##
```

If you have experience of working with Jenkins or other build automation server then you can find this blog post helpful otherwise it may be difficult to understand. This post is actually written focusing on Jenkins pipeline stages to implement automated application packaging using docker without Dockerfile!
